# 分布式发布订阅消息系统Kafka

## 概述
### 1.官网概念
a distributed streaming platform (分布式流处理平台)
    
    1.PUBLIC & SUBSCRIBE （类似发布订阅消息系统）
    2.PROCESS (高效实时流处理数据)
    3.STORE (安全多副本在分布式系统中存储)

Kafka is used for building read-time data pipelines and streaming apps. It is horizontally scalable,fault-tolerant,wicked fast,and runs in production in thousands of companies.

### 2.Kafka架构

    1.producer:特指消息的生产者
    2.consumer:特指消息的消费者
    3.broker:缓存代理，Kafa集群中的一台或多台服务器统称为broker
    4.topic:主题，特指 Kafka 处理的消息源（feeds of messages）的不同分类

## 安装

### 先安装zookeeper
1.tar -zxvf 解压

2.配置环境变量
```bash
export ZK_HOME=/home/hadoop/app/zookeeper-3.4.5-cdh5.7.0
export PATH=$ZK_HOME/bin:$PATH
```
3.配置conf/zoo.cfg
```bash
dataDir=/home/hadoop/app/zookeeper-3.4.5-cdh5.7.0/tmp
```
4.启动 `cd $ZK_HOME/bin && ./zkServer.sh start`,客户端连接 `cd $ZK_HOME/bin && ./zkCli.sh`

### 再安装Kafka
1.老惯例，解压。

2.配置环境变量
```bash
export KAFKA_HOME=/home/hadoop/app/kafka_2.11-0.9.0.0
export PATH=$KAFKA_HOME/bin:$PATH
```
3.配置注意点
```bash
***file:$KAFKA_HOME/config/server.properties***

broker.id=0 #唯一ID,一个实例一个编号
listeners=PLAINTEXT://:9092 #监听端口
host.name=localhost #指定一个地址
log.dirs=/home/hadoop/app/kafka_2.11-0.9.0.0/logs #配置日志地址，默认tmp会被清空
num.partitions=1 # 分区数量
zookeeper.connect=localhost:2181 #配置zk的地址
```
4.启动
```bash
kafka-server-start.sh $KAFKA_HOME/config/server.properties
```
5.创建一个topic，需要指定zookeeper
```bash
bin/kafka-topics.sh --create --zookeeper localhost:2181 --replication-factor 1 --partitions 1 --topic hello_topic
```
6.查看topic
```bash
bin/kafka-topics.sh --list --zookeeper localhost:2181
```
7.可以生产消息了，需要制定broker
```bash
bin/kafka-console-producer.sh --broker-list localhost:9092 --topic hello_topic
```
8.需要配置一个消费者来接收这个消息，需要指定zookeeper
```bash
bin/kafka-console-consumer.sh --zookeeper localhost:2181 --topic hello_topic --from-beginning

#注：--from-beginning 带这个参数之后，重新启动再进行消费会接着上一次的消费内容，不带则反之。
```
9.还有许多命令，查看help就好了。

### 单节点多broker部署和使用,直接看官方文档吧。
First we make a config file for each of the brokers (on Windows use the copy command instead):（复制三份配置文件出来）
    
    > cp config/server.properties config/server-1.properties
    > cp config/server.properties config/server-2.properties
    > cp config/server.properties config/server-3.properties

Now edit these new files and set the following properties:（编辑配置，修改broker.id和端口和日志地址）
    
    config/server-1.properties:
    broker.id=1
    listeners=PLAINTEXT://:9093
    log.dir=/tmp/kafka-logs-1
    
    config/server-2.properties:
    broker.id=2
    listeners=PLAINTEXT://:9094
    log.dir=/tmp/kafka-logs-2

    config/server-3.properties:
    broker.id=3
    listeners=PLAINTEXT://:9095
    log.dir=/tmp/kafka-logs-3

The broker.id property is the unique and permanent name of each node in the cluster. We have to override the port and log directory only because we are running these all on the same machine and we want to keep the brokers from all trying to register on the same port or overwrite each other's data.（id必须为不重复等等等等等.....）

We already have Zookeeper, so we just need to start the three new nodes:（保证zk启动，然后运行命令启动kafka）

    > bin/kafka-server-start.sh config/server-1.properties &
    ...
    > bin/kafka-server-start.sh config/server-2.properties &
    ...
    > bin/kafka-server-start.sh config/server-3.properties &
    ...

Now create a new topic with a replication factor of three:（创建topic主题）
```bash
bin/kafka-topics.sh --create --zookeeper localhost:2181 --replication-factor 3 --partitions 1 --topic double-topic
```

Okay but now that we have a cluster how can we know which broker is doing what? To see that run the "describe topics" command:（查看一下describe详细情况）
```bash
#command
bin/kafka-topics.sh --describe --zookeeper localhost:2181 --topic my-replicated-topic

#result
Topic:my-replicated-topic   PartitionCount:1    ReplicationFactor:3 Configs:
Topic: my-replicated-topic  Partition: 0    Leader: 1   Replicas: 1,2,0 Isr: 1,2,0
```

"leader" is the node responsible for all reads and writes for the given partition. Each node will be the leader for a randomly selected portion of the partitions.

"replicas" is the list of nodes that replicate the log for this partition regardless of whether they are the leader or even if they are currently alive.

"isr" is the set of "in-sync" replicas. This is the subset of the replicas list that is currently alive and caught-up to the leader.

Let's publish a few messages to our new topic:
```bash
#发布一个消息
bin/kafka-console-producer.sh --broker-list localhost:9092 --topic double-topic
```

Now let's consume these messages:
```bash
#消费一个消息
bin/kafka-console-consumer.sh --zookeeper localhost:2181 --from-beginning --topic double-topic
```

然后可以完整发送接收消息了

Now let's test out fault-tolerance. Broker 1 was acting as the leader so let's kill it:(准备杀死一个kafka)
```bash
ps aux | grep server-1.properties
kill -9 7564
```
Leadership has switched to one of the slaves and node 1 is no longer in the in-sync replica set:
```bash
bin/kafka-topics.sh --describe --zookeeper localhost:2181 --topic double-topic

Topic: my-replicated-topic  Partition: 0    Leader: 2   Replicas: 1,2,0 Isr: 2,3 (1 已经不在了，消息还是能正常发送接收)
```

## 通过Java调用API使用

### 项目创建
1. open IDEA,and click `Create Project --> Maven --> checked Create from archetype`
2. choose `scala-archetype-simple` and fill the gav, waiting project build.
3. 配置一下`maven`镜像吧
```xml
<mirror> 
    <id>alimaven</id> 
    <name>aliyun maven</name> 
    <url>http://maven.aliyun.com/nexus/content/groups/public/</url> 
    <mirrorOf>central</mirrorOf> 
</mirror> 
```
4. 配置pom.xml文件，修改scala.version为2.11.8,并且添加kafka依赖
```xml
<kafka.version>0.9.0.0</kafka.version>

<!--Kafka依赖-->
<dependency>
    <groupId>org.apache.kafka</groupId>
    <artifactId>kafka_2.11</artifactId>
    <version>${kafka.version}</version>
</dependency>
```
5.创建我的kafka包就创建ok了 `com.whoiszxl.spark.kafka`

### 生产者代码编写
1.创建kafka配置文件
```java

public class KafkaProperties {

    public static final String ZK = "120.78.72.189:2181";

    public static final String TOPIC = "hello_topic";

    public static final String BROKER_LIST = "120.78.72.189:9092";
}
```
2.创建生产者代码
```java
public class KafkaProducer extends Thread {

    private String topic;

    private Producer<Integer, String> producer;

    public KafkaProducer(String topic) {
        this.topic = topic;
        Properties properties = new Properties();
        properties.put("metadata.broker.list",KafkaProperties.BROKER_LIST);
        properties.put("serializer.class","kafka.serializer.StringEncoder");
        properties.put("request.required.acks","1");
        producer = new Producer<Integer, String>(new ProducerConfig(properties));
    }

    @Override
    public void run() {
        int messageNo = 1;

        while (true) {
            String message = "message_" + messageNo;
            //producer.send(new KeyedMessage<Integer, String>(topic, message));
            producer.send(new KeyedMessage<Integer, String>(topic, message));
            System.out.println("Send :" + message);
            messageNo++;
            try {
                Thread.sleep(2000);
            } catch (InterruptedException e) {
                e.printStackTrace();
            }
        }
    }
}
```
3.实际调用
```java
public class KafkaClientApp {
    public static void main(String[] args) {
        new KafkaProducer(KafkaProperties.TOPIC).start();
    }
}
```
4.注意事项！！！！！！！！！！！！
```java
Exception in thread "main" Failed to send requests for topics test with correlation ids in [0,12]
kafka.common.FailedToSendMessageException: Failed to send messages after 3 tries.
```
出现这个异常，需要配置server.properties的 `advertised.host.name`为当前服务器的地址，其默认的是localhost，远程连接会出问题。

### 消费者代码编写
1.消费者代码
```java
public class KafkaConsumer extends Thread {

    private String topic;

    public KafkaConsumer(String topic) {
        this.topic = topic;
    }

    private ConsumerConnector createConnector() {

        Properties properties = new Properties();
        properties.put("zookeeper.connect", KafkaProperties.ZK);
        properties.put("group.id",KafkaProperties.GROUP_ID);
        return Consumer.createJavaConsumerConnector(new ConsumerConfig(properties));
    }

    @Override
    public void run() {
        ConsumerConnector consumer = createConnector();

        Map<String, Integer> topicCountMap = new HashMap<String, Integer>();
        topicCountMap.put(topic, 1);
        // List<KafkaStream<byte[], byte[]>>  对应的数据流
        Map<String, List<KafkaStream<byte[], byte[]>>> messageStream = consumer.createMessageStreams(topicCountMap);

        KafkaStream<byte[], byte[]> stream = messageStream.get(topic).get(0);   //获取我们每次接收到的数据
        ConsumerIterator<byte[], byte[]> iterator = stream.iterator();

        while (iterator.hasNext()) {
            String message = new String(iterator.next().message());
            System.out.println("receive: " + message);
        }
    }
}
```
2.KafkaProperties需要配置组ID
```java
public static final String GROUP_ID = "test-group1";
```
