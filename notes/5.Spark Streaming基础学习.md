# Spark Streaming基础学习

文档地址：[http://spark.apache.org/docs/latest/streaming-programming-guide.html]

## 概述
Spark Streaming is an extension of the core Spark API that enables scalable, high-throughput, fault-tolerant stream processing of live data streams. `(可扩展，高吞吐，容错)`Data can be ingested from many sources like Kafka, Flume, Kinesis, or TCP sockets,`(可以加入多种数据源)`and can be processed using complex algorithms expressed with high-level functions like map, reduce, join and window. `(可以用多种算法)`Finally, processed data can be pushed out to filesystems, databases, and live dashboards.`(可以将结果推送到文件系统或数据库)`In fact, you can apply Spark’s machine learning and graph processing algorithms on data streams.`(还可以做机器学习和图形处理哦！)`

### 个人概述
将来自各种数据源（flume，kafka，file system，tcp...）经过spark streaming 处理后将结果输出到外部文件系统(database,nosql....)

## 统计网络词频
### 创建一个网络发送端口
`nc -lk 9999`

### 执行Spark Streaming的词频统计jar
```bash
#生产环境使用spark-submit来提交统计脚本

./spark-submit --master local[2] \
--class org.apache.spark.examples.streaming.NetworkWordCount \
--name NetworkWorkCount \
/home/hadoop/app/spark-2.2.0-bin-2.6.0-cdh5.7.0/examples/jars/spark-examples_2.11-2.2.0.jar localhost 9999
```

```scala
//测试环境使用spark-shell来提交
./spark-shell --master local[2] #启动


//控制台执行代码
import org.apache.spark.streaming.{Seconds, StreamingContext}
val ssc = new StreamingContext(sc, Seconds(1))
val lines = ssc.socketTextStream("localhost", 9999)
val words = lines.flatMap(_.split(" "))
val wordCounts = words.map(x => (x, 1)).reduceByKey(_ + _)
wordCounts.print()
ssc.start()
ssc.awaitTermination()
```

## 工作原理
    粗粒度
    1. batches of x seconds：按秒将数据流进行拆分
    2. 传入Spark Streaming,再传到底层Spark进行处理
    3. 再proessed result处理结果


    细粒度
    1. Spark Streaming 的Application具有SaprkContext的上下文对象，其上还有StreamingContext上下文对象
    2. 其driver驱动会运行Receivers接收器到Executor中，其接收器会接收InputStream输入流，其中的数据会做replicate blocks副本，都在Executor的内存中
    3. Receivers 会发送块的分割报告回去Application，StreamingContext会发送jobs给SparkContext，SparkContext再将处理块的任务分发给其它的Executor，通过设置的秒数返回记录。


## 核心编程

### 概念 (翻译下官方文档)

#### Initializing StreamingContext
To initialize a Spark Streaming program, a StreamingContext object has to be created which is the main entry point of all Spark Streaming functionality.`（初始化一个StreamingContext，其可以创建一个主入口点可以创建所有的Spark Streaming的功能）`

A StreamingContext object can be created from a SparkConf object.`(一个StreamingContext可以通过SparkConf对象创建)`
```scala
import org.apache.spark._
import org.apache.spark.streaming._

/*
 *The appName parameter is a name for your application to show on the cluster UI. 
 *appName参数是展示在webUI界面上的应用名称
 *
 *master is a Spark, Mesos or YARN cluster URL, or a special “local[*]” string to run in local mode. 
 *master参数是填写你的yarn地址等等，或者填写`local[*]`启动一个本地模式
 *
 *when running on a cluster, you will not want to hardcode master in the program, but rather launch the 
 *application with spark-submit and receive it there.
 *当你运行在集群中的时候，不应该把master写死，应该写在spark-submit的参数中.
 *
 *for local testing and unit tests, you can pass “local[*]” to run Spark Streaming in-process (detects the number of cores in the local system). 
 *本地测试模式的时候，通过`local[*]`去运行 （通过你服务器的核心数来指定）
 */
val conf = new SparkConf().setAppName(appName).setMaster(master)

/*
 *The batch interval must be set based on the latency requirements of your application and available cluster resources.
 *要通过你的需求设置批处理间隔
 */
val ssc = new StreamingContext(conf, Seconds(1))
```

##### 注意事项
After a context is defined, you have to do the following.

1. Define the input sources by creating input DStreams.`（可以创建input DStreams）`
2. Define the streaming computations by applying transformation and output operations to DStreams.`(应用transformation和输出到DStreams)`
3. Start receiving data and processing it using streamingContext.start().`(使用streamingContext.start()接收和处理数据)`
4. Wait for the processing to be stopped (manually or due to any error) using streamingContext.awaitTermination().`(等待处理到停止)`
5. The processing can be manually stopped using streamingContext.stop().`（通过streamingContext.stop()手动停止）`

Points to remember:
1. Once a context has been started, no new streaming computations can be set up or added to it.`(一旦context启动了，新的计算就不能被设置和添加了)`
2. Once a context has been stopped, it cannot be restarted.`(一旦停止了，就不能再使用streamingContext.start()了)`
3. Only one StreamingContext can be active in a JVM at the same time.`(一个StreamingContext只能在一个jvm上)`
4. stop() on StreamingContext also stops the SparkContext. To stop only the StreamingContext, set the optional parameter of stop() called stopSparkContext to false.`（给stopSparkContext设置false参数可以仅停止StreamingContext）`
5. A SparkContext can be re-used to create multiple StreamingContexts, as long as the previous StreamingContext is stopped (without stopping the SparkContext) before the next StreamingContext is created.`(一个SparkContext可以创建多个StreamingContexts，只要之前的StreamingContext停止了，但不能停止SparkContext)`


#### Discretized Streams (DStreams)
Discretized Stream or DStream is the basic abstraction provided by Spark Streaming.`(DStream是Spark Streaming的抽象的一个基础)` It represents a continuous stream of data, either the input data stream received from source, or the processed data stream generated by transforming the input stream. `(其代表了一个数据流，数据来源来自数据源头，或者来自另一个DStream通过transforming转换了的)`Internally, ```a DStream is represented by a continuous series of RDDs,（一个DSream代表了一个连续的RDDs数据集）``` which is Spark’s abstraction of an immutable, distributed dataset (see Spark Programming Guide for more details). `（RDDs是一个spark的抽象的，不可变的分布式的数据集）`Each RDD in a DStream contains data from a certain interval, as shown in the following figure.`（rdd 在dstream内容中包含的数据是一个批次的时间间隔）`

Any operation applied on a DStream translates to operations on the underlying RDDs. For example, in the earlier example[http://spark.apache.org/docs/latest/streaming-programming-guide.html#a-quick-example] of converting a stream of lines to words, the flatMap operation is applied on each RDD in the lines DStream to generate the RDDs of the words DStream. This is shown in the following figure.

`对于DStream操作,比如flatMap,其底层会被翻译为对DStream中由时间间隔分割的RDD做相同的操作，因为DStream就是由不同时间批次的RDD构成的。`

**总的来说，就是个多个批次构成的流。**

#### Input DStreams and Receivers
    Input DStreams are DStreams representing the stream of input data received from streaming sources.`(Input DStreams 是一个从数据源接收数据的流)` In the quick example[http://spark.apache.org/docs/latest/streaming-programming-guide.html#a-quick-example], lines was an input DStream as it represented the stream of data received from the netcat server.`(例子中lines就是input Dstream，其来自netcat的服务)` Every input DStream (except file stream, discussed later in this section) is associated with a Receiver (Scala doc, Java doc) object which receives the data from a source and stores it in Spark’s memory for processing.`(每一个input DStream 都需要一个Receiver，除了文件系统的)`

Spark Streaming provides two categories of built-in streaming sources.

    1. Basic sources: Sources directly available in the StreamingContext API. Examples: file systems, and socket connections.(基础数据源，比如文件系统，socket)
    2. Advanced sources: Sources like Kafka, Flume, Kinesis, etc. are available through extra utility classes. These require linking against extra dependencies as discussed in the linking section.(高级数据源，比如kafka，flume等)

注意点：

When running a Spark Streaming program locally, do not use “local” or “local[1]” as the master URL. Either of these means that only one thread will be used for running tasks locally. If you are using an input DStream based on a receiver (e.g. sockets, Kafka, Flume, etc.), then the single thread will be used to run the receiver, leaving no thread for processing the received data. Hence, when running locally, always use “local[n]” as the master URL, where n > number of receivers to run (see Spark Properties for information on how to set the master).`(运行本地环境的话，一定要local[n]>1,因为dstream和receiver都需要一个线程，1显然是不行的)`

#### Transformations on DStreams
Similar to that of RDDs, transformations allow the data from the input DStream to be modified. DStreams support many of the transformations available on normal Spark RDD’s. Some of the common ones are as follows.`(与RDDs类似，转换数据从input Stream修改，DStreams支持很多种转换方式)`

例如：map(func),flatMap(func),filter(func),count()等等等等

#### Output Operations on DStreams
Output operations allow DStream’s data to be pushed out to external systems like a database or a file systems. Since the output operations actually allow the transformed data to be consumed by external systems, they trigger the actual execution of all the DStream transformations (similar to actions for RDDs). Currently, the following output operations are defined:`(Output 可以将数据推送到外部系统比如数据库，文件系统)`

例如：print()，saveAsTextFiles(prefix, [suffix]),saveAsObjectFiles(prefix, [suffix]),saveAsHadoopFiles(prefix, [suffix]),foreachRDD(func)

## 案例 Coding

### 处理socket接收的数据
添加两个依赖
```xml
<dependency>
    <groupId>com.fasterxml.jackson.module</groupId>
    <artifactId>jackson-module-scala_2.11</artifactId>
    <version>2.6.5</version>
</dependency>

<dependency>
    <groupId>net.jpountz.lz4</groupId>
    <artifactId>lz4</artifactId>
    <version>1.3.0</version>
</dependency>
```
编写scala代码
```scala
/**
  * spark Streaming 处理sorket数据
  */
object NetworkWordCount {

  def main(args: Array[String]): Unit = {
    //1.创建一个spark配置类,配置本地运行，配置UI界面显示的name
    val sparkConf = new SparkConf().setMaster("local[2]").setAppName("NetworkWordCount");

    //2.创建spark streaming的上下文对象,传入配置和批处理时间间隔为3秒
    val ssc = new StreamingContext(sparkConf,Seconds(3))

    //3.通过tcp获取到数据
    val lines = ssc.socketTextStream("120.78.72.189",6666)

    //4.通过空格进行分割
    val result = lines.flatMap(_.split(" ")).map((_,1)).reduceByKey(_+_)

    result.print()

    ssc.start()
    ssc.awaitTermination()
  }
}
```
#### 原理
1. linux服务器的`nc -lk 6666`开启一个socket流
2. Receiver占用一个local[1]，接收6666端口的数据，存到memory内存中
3. 再启用一个local[1]，执行`Transformations的map flatMap`等操作

### 处理文件接收的数据
```scala
/**
  * 通过Spark Streaming处理文件系统
  */
object FileWordCount {

  def main(args: Array[String]): Unit = {

    //1.文件系统的只需要一个local就可以了
    val sparkConf = new SparkConf().setMaster("local").setAppName("FileWordCount")
    val ssc = new StreamingContext(sparkConf, Seconds(3))

    //windows下的文件路径还有点问题
    val lines = ssc.textFileStream("file:///home/hadoop/aa")

    val result = lines.flatMap(_.split(" ")).map((_, 1)).reduceByKey(_ + _)

    result.print()

    ssc.start()
    ssc.awaitTermination()
  }
}
```