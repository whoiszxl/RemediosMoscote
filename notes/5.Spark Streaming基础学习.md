# Spark Streaming基础学习

文档地址：[http://spark.apache.org/docs/latest/streaming-programming-guide.html]

## 概述
Spark Streaming is an extension of the core Spark API that enables scalable, high-throughput, fault-tolerant stream processing of live data streams. `(可扩展，高吞吐，容错)`Data can be ingested from many sources like Kafka, Flume, Kinesis, or TCP sockets,`(可以加入多种数据源)`and can be processed using complex algorithms expressed with high-level functions like map, reduce, join and window. `(可以用多种算法)`Finally, processed data can be pushed out to filesystems, databases, and live dashboards.`(可以将结果推送到文件系统或数据库)`In fact, you can apply Spark’s machine learning and graph processing algorithms on data streams.`(还可以做机器学习和图形处理哦！)`

### 个人概述
将来自各种数据源（flume，kafka，file system，tcp...）经过spark streaming 处理后将结果输出到外部文件系统(database,nosql....)

## 统计网络词频
### 创建一个网络发送端口
`nc -lk 9999`

### 执行Spark Streaming的词频统计jar
```bash
#生产环境使用spark-submit来提交统计脚本

./spark-submit --master local[2] \
--class org.apache.spark.examples.streaming.NetworkWordCount \
--name NetworkWorkCount \
/home/hadoop/app/spark-2.2.0-bin-2.6.0-cdh5.7.0/examples/jars/spark-examples_2.11-2.2.0.jar localhost 9999
```

```scala
//测试环境使用spark-shell来提交
./spark-shell --master local[2] #启动


//控制台执行代码
import org.apache.spark.streaming.{Seconds, StreamingContext}
val ssc = new StreamingContext(sc, Seconds(1))
val lines = ssc.socketTextStream("localhost", 9999)
val words = lines.flatMap(_.split(" "))
val wordCounts = words.map(x => (x, 1)).reduceByKey(_ + _)
wordCounts.print()
ssc.start()
ssc.awaitTermination()
```

## 工作原理
    粗粒度
    1. batches of x seconds：按秒将数据流进行拆分
    2. 传入Spark Streaming,再传到底层Spark进行处理
    3. 再proessed result处理结果


    细粒度
    1. Spark Streaming 的Application具有SaprkContext的上下文对象，其上还有StreamingContext上下文对象
    2. 其driver驱动会运行Receivers接收器到Executor中，其接收器会接收InputStream输入流，其中的数据会做replicate blocks副本，都在Executor的内存中
    3. Receivers 会发送块的分割报告回去Application，StreamingContext会发送jobs给SparkContext，SparkContext再将处理块的任务分发给其它的Executor，通过设置的秒数返回记录。

