# Spark Streaming基础学习

文档地址：[http://spark.apache.org/docs/latest/streaming-programming-guide.html]

## 概述
Spark Streaming is an extension of the core Spark API that enables scalable, high-throughput, fault-tolerant stream processing of live data streams. `(可扩展，高吞吐，容错)`Data can be ingested from many sources like Kafka, Flume, Kinesis, or TCP sockets,`(可以加入多种数据源)`and can be processed using complex algorithms expressed with high-level functions like map, reduce, join and window. `(可以用多种算法)`Finally, processed data can be pushed out to filesystems, databases, and live dashboards.`(可以将结果推送到文件系统或数据库)`In fact, you can apply Spark’s machine learning and graph processing algorithms on data streams.`(还可以做机器学习和图形处理哦！)`

### 个人概述
将来自各种数据源（flume，kafka，file system，tcp...）经过spark streaming 处理后将结果输出到外部文件系统(database,nosql....)

## 统计网络词频
### 创建一个网络发送端口
`nc -lk 9999`

### 执行Spark Streaming的词频统计jar
```bash
#生产环境使用spark-submit来提交统计脚本

./spark-submit --master local[2] \
--class org.apache.spark.examples.streaming.NetworkWordCount \
--name NetworkWorkCount \
/home/hadoop/app/spark-2.2.0-bin-2.6.0-cdh5.7.0/examples/jars/spark-examples_2.11-2.2.0.jar localhost 9999
```

```scala
//测试环境使用spark-shell来提交
./spark-shell --master local[2] #启动


//控制台执行代码
import org.apache.spark.streaming.{Seconds, StreamingContext}
val ssc = new StreamingContext(sc, Seconds(1))
val lines = ssc.socketTextStream("localhost", 9999)
val words = lines.flatMap(_.split(" "))
val wordCounts = words.map(x => (x, 1)).reduceByKey(_ + _)
wordCounts.print()
ssc.start()
ssc.awaitTermination()
```

## 工作原理
    粗粒度
    1. batches of x seconds：按秒将数据流进行拆分
    2. 传入Spark Streaming,再传到底层Spark进行处理
    3. 再proessed result处理结果


    细粒度
    1. Spark Streaming 的Application具有SaprkContext的上下文对象，其上还有StreamingContext上下文对象
    2. 其driver驱动会运行Receivers接收器到Executor中，其接收器会接收InputStream输入流，其中的数据会做replicate blocks副本，都在Executor的内存中
    3. Receivers 会发送块的分割报告回去Application，StreamingContext会发送jobs给SparkContext，SparkContext再将处理块的任务分发给其它的Executor，通过设置的秒数返回记录。


## 核心编程

### 概念 (翻译下官方文档)

#### Initializing StreamingContext
To initialize a Spark Streaming program, a StreamingContext object has to be created which is the main entry point of all Spark Streaming functionality.`（初始化一个StreamingContext，其可以创建一个主入口点可以创建所有的Spark Streaming的功能）`

A StreamingContext object can be created from a SparkConf object.`(一个StreamingContext可以通过SparkConf对象创建)`
```scala
import org.apache.spark._
import org.apache.spark.streaming._

/*
 *The appName parameter is a name for your application to show on the cluster UI. 
 *appName参数是展示在webUI界面上的应用名称
 *
 *master is a Spark, Mesos or YARN cluster URL, or a special “local[*]” string to run in local mode. 
 *master参数是填写你的yarn地址等等，或者填写`local[*]`启动一个本地模式
 *
 *when running on a cluster, you will not want to hardcode master in the program, but rather launch the 
 *application with spark-submit and receive it there.
 *当你运行在集群中的时候，不应该把master写死，应该写在spark-submit的参数中.
 *
 *for local testing and unit tests, you can pass “local[*]” to run Spark Streaming in-process (detects the number of cores in the local system). 
 *本地测试模式的时候，通过`local[*]`去运行 （通过你服务器的核心数来指定）
 */
val conf = new SparkConf().setAppName(appName).setMaster(master)

/*
 *The batch interval must be set based on the latency requirements of your application and available cluster resources.
 *要通过你的需求设置批处理间隔
 */
val ssc = new StreamingContext(conf, Seconds(1))
```

##### 注意事项
After a context is defined, you have to do the following.

1. Define the input sources by creating input DStreams.`（可以创建input DStreams）`
2. Define the streaming computations by applying transformation and output operations to DStreams.`(应用transformation和输出到DStreams)`
3. Start receiving data and processing it using streamingContext.start().`(使用streamingContext.start()接收和处理数据)`
4. Wait for the processing to be stopped (manually or due to any error) using streamingContext.awaitTermination().`(等待处理到停止)`
5. The processing can be manually stopped using streamingContext.stop().`（通过streamingContext.stop()手动停止）`

Points to remember:
1. Once a context has been started, no new streaming computations can be set up or added to it.`(一旦context启动了，新的计算就不能被设置和添加了)`
2. Once a context has been stopped, it cannot be restarted.`(一旦停止了，就不能再使用streamingContext.start()了)`
3. Only one StreamingContext can be active in a JVM at the same time.`(一个StreamingContext只能在一个jvm上)`
4. stop() on StreamingContext also stops the SparkContext. To stop only the StreamingContext, set the optional parameter of stop() called stopSparkContext to false.`（给stopSparkContext设置false参数可以仅停止StreamingContext）`
5. A SparkContext can be re-used to create multiple StreamingContexts, as long as the previous StreamingContext is stopped (without stopping the SparkContext) before the next StreamingContext is created.`(一个SparkContext可以创建多个StreamingContexts，只要之前的StreamingContext停止了，但不能停止SparkContext)`


#### Discretized Streams (DStreams)
Discretized Stream or DStream is the basic abstraction provided by Spark Streaming.`(DStream是Spark Streaming的抽象的一个基础)` It represents a continuous stream of data, either the input data stream received from source, or the processed data stream generated by transforming the input stream. `(其代表了一个数据流，数据来源来自数据源头，或者来自另一个DStream通过transforming转换了的)`Internally, ```a DStream is represented by a continuous series of RDDs,（一个DSream代表了一个连续的RDDs数据集）``` which is Spark’s abstraction of an immutable, distributed dataset (see Spark Programming Guide for more details). `（RDDs是一个spark的抽象的，不可变的分布式的数据集）`Each RDD in a DStream contains data from a certain interval, as shown in the following figure.`（rdd 在dstream内容中包含的数据是一个批次的时间间隔）`

Any operation applied on a DStream translates to operations on the underlying RDDs. For example, in the earlier example[http://spark.apache.org/docs/latest/streaming-programming-guide.html#a-quick-example] of converting a stream of lines to words, the flatMap operation is applied on each RDD in the lines DStream to generate the RDDs of the words DStream. This is shown in the following figure.

`对于DStream操作,比如flatMap,其底层会被翻译为对DStream中由时间间隔分割的RDD做相同的操作，因为DStream就是由不同时间批次的RDD构成的。`

**总的来说，就是个多个批次构成的流。**