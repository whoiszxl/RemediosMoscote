# 9.Spark Streaming+Flume+Kafka搭建通用流处理平台


## 配置 Log4j
```java
/**
 * Created by zxlvoid on 2017/12/27 0027.
 * 日志生产
 */
public class LoggerGenerator {

    private static Logger logger = Logger.getLogger(LoggerGenerator.class.getName());

    public static void main(String[] args) throws InterruptedException {

        int index = 0;
        while (true) {
            Thread.sleep(1000);
            logger.info("current number is:" + index++);
        }
    }

}
```

```properties
vim log4j.properties

log4j.rootLogger=INFO,stdout,flume

log4j.appender.stdout = org.apache.log4j.ConsoleAppender
log4j.appender.stdout.target = System.out
log4j.appender.stdout.layout=org.apache.log4j.PatternLayout
log4j.appender.stdout.layout.ConversionPattern=%d{yyyy-MM-dd HH:mm:ss,SSS} [%t] [%c] [%p] - %m%n
```

## 整合日志输出到Flume

vim streaming
```conf
agent1.sources=avro-source
agent1.channels=logger-channel
agent1.sinks=log-sink

#define source
agent1.sources.avro-source.type=avro
#配置到服务器上的时候需要将localhost换成ifconfig中的ip地址，不然会报rpc连接不上的错误
agent1.sources.avro-source.bind=localhost
agent1.sources.avro-source.port=41414

#define channel
agent1.channels.logger-channel.type=memory

#define sink
agent1.sinks.log-sink.type=logger

agent1.sources.avro-source.channels=logger-channel
agent1.sinks.log-sink.channel=logger-channel
```

启动flume
```bash
flume-ng agent \
--conf $FLUME_HOME/conf \
--conf-file $FLUME_HOME/conf/streaming.conf \
--name agent1 \
-Dflume.root.logger=INFO,console
```

```conf
# log4j.properties还要加一段配置

log4j.appender.flume = org.apache.flume.clients.log4jappender.Log4jAppender
log4j.appender.flume.Hostname = 120.78.72.189
log4j.appender.flume.Port = 41414
log4j.appender.flume.UnsafeMode = true
log4j.appender.flume.layout=org.apache.log4j.PatternLayout
```


## 整合Flume到Kafka

1. 启动Kafka服务：`./bin/kafka-server-start.sh -daemon $KAFKA_HOME/config/server.properties`

2. 创建一个测试用的主题：`./kafka-topics.sh --create --zookeeper localhost:2181 --replication-factor 1 --partitions 1 --topic streaming_topic`

3. 编辑flume配置文件：vim streaming2.conf
```conf
agent1.sources=avro-source
agent1.channels=logger-channel
agent1.sinks=kafka-sink

#define source
agent1.sources.avro-source.type=avro
#配置到服务器上的时候需要将localhost换成ifconfig中的ip地址，不然会报rpc连接不上的错误 192.168.1.8
agent1.sources.avro-source.bind=localhost
agent1.sources.avro-source.port=41414

#define channel
agent1.channels.logger-channel.type=memory

#define sink
agent1.sinks.kafka-sink.type=org.apache.flume.sink.kafka.KafkaSink
agent1.sinks.kafka-sink.topic=streaming_topic
agent1.sinks.kafka-sink.brokerList=120.78.72.189:9092
agent1.sinks.kafka-sink.requiredAsks=1
#20条记录为一个批次
agent1.sinks.kafka-sink.batchSize=20

agent1.sources.avro-source.channels=logger-channel
agent1.sinks.kafka-sink.channel=logger-channel
```

启动flume
```bash
flume-ng agent \
--conf $FLUME_HOME/conf \
--conf-file $FLUME_HOME/conf/streaming2.conf \
--name agent1 \
-Dflume.root.logger=INFO,console
```


## Spark Streaming对接收到的数据进行处理

编辑一个scala代码
```scala
object KafkaStreaming {

  def main(args: Array[String]): Unit = {

    if (args.length != 4) {
      System.err.println("params have mistakes")
    }

    var Array(zkQuorum, group, topics, numThreads) = args

    var sparkConf = new SparkConf() //.setAppName("KafkaReceiverWordCount").setMaster("local[2]")

    var ssc = new StreamingContext(sparkConf, Seconds(5))

    val topicMap = topics.split(",").map((_, numThreads.toInt)).toMap

    val messages = KafkaUtils.createStream(ssc, zkQuorum, group, topicMap)

    messages.map(_._2).count().print()

    ssc.start()
    ssc.awaitTermination()

  }
}

```

6. `mvn clean package -DskipTests`打包后,rz到服务器,执行spark-submit
```bash
spark-submit \
--class com.whoiszxl.spark.KafkaStreaming \
--master local[2] \
--name KafkaStreaming \
--packages org.apache.spark:spark-streaming-kafka-0-8_2.11:2.2.0 \
/home/hadoop/lib/sparktrain-1.0.jar localhost:2181 test streaming_topic 1
```




