# 项目实战

## 需求说明
1. 统计今天到现在为止的课程访问数量
2. 统计今天到现在为止从搜索引擎过来的实战课程的访问量


## 实时日志模拟

### 编写一个python脚本
```python
#coding=UTF-8

#导入随机数和时间库
import random
import time

#定义模拟url的域名
my_url = "http://whoiszxl.com/"

#需要随机拼接的uri
url_paths = [
    "class/112.html",
    "class/128.html",
    "class/145.html",
    "class/146.html",
    "class/131.html",
    "class/130.html",
    "class/145.html",
    "learn/888",
    "learn/666",
    "course/list",
    "note/list"
]

#随机生成的域名数字
ip_slices = [123,156,122,23,56,33,10,100,102,33,56,89,66,77,88,99,200,201,202,203,204,205,205,206,207,208,209,210,220,230,235]

#referer源地址
http_referers = [
    "https://www.baidu.com/s?wd={query}",
    "https://www.google.com/search?q={query}",
    "https://search.yahoo.com/search?q={query}",
    "https://bing.com/search?q={query}",
]

#原地址搜索的关键字，填充到query用
search_keyword = [
    "wangjie",
    "红拂夜奔",
    "黄金时代",
    "JAVA编程思想",
    "绿毛水怪",
    "朝花夕拾"
]

#随机生成状态码
status_codes = ["200","404","500","200","200","200","200","200"]

#随机生成状态码函数 sample(param1,param2) param1:需要随机生成的数组，param2:随机生成的个数
def sample_status_code():
    return random.sample(status_codes,1)[0]

def sample_url():
    return random.sample(url_paths,1)[0]

def sample_ip():
    slice = random.sample(ip_slices,4)
    return ".".join([str(item) for item in slice])

def sample_referer():
    if random.uniform(0,1) > 0.2:
        return "-"

    refer_str = random.sample(http_referers,1)
    query_str = random.sample(search_keyword,1)
    return refer_str[0].format(query=query_str[0])

def generate_log(count = 10):
    time_str = time.strftime("%Y-%m-%d %H:%M:%S",time.localtime())

    f = open("/home/hadoop/python/logs/access.log","w+")

    while count >= 1:
        query_log = "{ip}\t{local_time}\t\"GET /{site}{url} HTTP/1.1\"\t{referer}\t{status_code}".format(local_time=time_str,site=my_url,url=sample_url(),ip=sample_ip(),referer=sample_referer(),status_code=sample_status_code())
        print query_log
        f.write(query_log + "\n")
        count = count - 1

if __name__=='__main__':
    generate_log(100)
```

### 使用crontab定时生成日志
1. 命令： crontab -e
2. 测试网址：[https://tool.lu/crontab]
3. 配置cron为：`*/1 * * * * * python /home/hadoop/python/generate_log.py`


## 使用Flume收集日志
vim streaming_project.conf
```conf
exec-memory-logger.sources = exec-source
exec-memory-logger.sinks = logger-sink
exec-memory-logger.channels = memory-channel

exec-memory-logger.sources.exec-source.type = exec
exec-memory-logger.sources.exec-source.command = tail -F /home/hadoop/python/logs/access.log
exec-memory-logger.sources.exec-source.shell = /bin/sh -c

exec-memory-logger.channels.memory-channel.type = memory

exec-memory-logger.sinks.logger-sink.type = logger

exec-memory-logger.sources.exec-source.channels = memory-channel
exec-memory-logger.sinks.logger-sink.channel = memory-channel

```
运行
```shell
flume-ng agent \
--name exec-memory-logger \
--conf $FLUME_HOME/conf \
--conf-file /home/hadoop/project/streaming_project.conf \
-Dflume.root.logger=INFO,console
```

## 将Flume数据转移到Kafka

vim streaming_project2.conf
```conf
exec-memory-kafka.sources = exec-source
exec-memory-kafka.sinks = kafka-sink
exec-memory-kafka.channels = memory-channel

exec-memory-kafka.sources.exec-source.type = exec
exec-memory-kafka.sources.exec-source.command = tail -F /home/hadoop/python/logs/access.log
exec-memory-kafka.sources.exec-source.shell = /bin/sh -c

exec-memory-kafka.channels.memory-channel.type = memory

exec-memory-kafka.sinks.kafka-sink.type = org.apache.flume.sink.kafka.KafkaSink
exec-memory-kafka.sinks.kafka-sink.brokerList = localhost:9092
exec-memory-kafka.sinks.kafka-sink.topic = streaming_topic
exec-memory-kafka.sinks.kafka-sink.batchSize = 5
exec-memory-kafka.sinks.kafka-sink.requiredAcks = 1

exec-memory-kafka.sources.exec-source.channels = memory-channel
exec-memory-kafka.sinks.kafka-sink.channel = memory-channel

```
运行
```shell
flume-ng agent \
--name exec-memory-kafka \
--conf $FLUME_HOME/conf \
--conf-file /home/hadoop/project/streaming_project2.conf \
-Dflume.root.logger=INFO,console
```

创建一个消费者进行测试：`./bin/kafka-console-consumer.sh --zookeeper localhost:2181 --topic streaming_topic`

## Spark Streaming 接收Kafka数据
```scala
object ZxlStreamingApp {

  def main(args: Array[String]): Unit = {

    if (args.length != 4) {
      System.err.print("params have mistake.must 4 param")
      System.exit(1)
    }

    val Array(zkQuorum, groupId, topics, numThreads) = args
    val sparkConf = new SparkConf().setAppName("ZxlStreamingApp").setMaster("local[2]")
    val ssc = new StreamingContext(sparkConf, Seconds(6))

    val topicMap = topics.split(",").map((_, numThreads.toInt)).toMap
    val messages = KafkaUtils.createStream(ssc, zkQuorum, groupId, topicMap)

    messages.map(_._2).count().print

    ssc.start()
    ssc.awaitTermination()
  }

}

```