# 项目实战

## 需求说明
1. 统计今天到现在为止的课程访问数量
2. 统计今天到现在为止从搜索引擎过来的实战课程的访问量

## 实时日志模拟

### 编写一个python脚本
```python
#coding=UTF-8

#导入随机数和时间库
import random
import time

#定义模拟url的域名
my_url = "http://whoiszxl.com/"

#需要随机拼接的uri
url_paths = [
    "class/112.html",
    "class/128.html",
    "class/145.html",
    "class/146.html",
    "class/131.html",
    "class/130.html",
    "class/145.html",
    "learn/888",
    "learn/666",
    "course/list",
    "note/list"
]

#随机生成的域名数字
ip_slices = [123,156,122,23,56,33,10,100,102,33,56,89,66,77,88,99,200,201,202,203,204,205,205,206,207,208,209,210,220,230,235]

#referer源地址
http_referers = [
    "https://www.baidu.com/s?wd={query}",
    "https://www.google.com/search?q={query}",
    "https://search.yahoo.com/search?q={query}",
    "https://bing.com/search?q={query}",
]

#原地址搜索的关键字，填充到query用
search_keyword = [
    "wangjie",
    "红拂夜奔",
    "黄金时代",
    "JAVA编程思想",
    "绿毛水怪",
    "朝花夕拾"
]

#随机生成状态码
status_codes = ["200","404","500","200","200","200","200","200"]

#随机生成状态码函数 sample(param1,param2) param1:需要随机生成的数组，param2:随机生成的个数
def sample_status_code():
    return random.sample(status_codes,1)[0]

def sample_url():
    return random.sample(url_paths,1)[0]

def sample_ip():
    slice = random.sample(ip_slices,4)
    return ".".join([str(item) for item in slice])

def sample_referer():
    if random.uniform(0,1) > 0.2:
        return "-"

    refer_str = random.sample(http_referers,1)
    query_str = random.sample(search_keyword,1)
    return refer_str[0].format(query=query_str[0])

def generate_log(count = 10):
    time_str = time.strftime("%Y-%m-%d %H:%M:%S",time.localtime())

    f = open("/home/hadoop/python/logs/access.log","w+")

    while count >= 1:
        query_log = "{ip}\t{local_time}\t\"GET /{site}{url} HTTP/1.1\"\t{referer}\t{status_code}".format(local_time=time_str,site=my_url,url=sample_url(),ip=sample_ip(),referer=sample_referer(),status_code=sample_status_code())
        print query_log
        f.write(query_log + "\n")
        count = count - 1

if __name__=='__main__':
    generate_log(100)
```

### 使用crontab定时生成日志
1. 命令： crontab -e
2. 测试网址：[https://tool.lu/crontab]
3. 配置cron为：`*/1 * * * * * python /home/hadoop/python/generate_log.py`


## 使用Flume收集日志
vim streaming_project.conf
```conf
exec-memory-logger.sources = exec-source
exec-memory-logger.sinks = logger-sink
exec-memory-logger.channels = memory-channel

exec-memory-logger.sources.exec-source.type = exec
exec-memory-logger.sources.exec-source.command = tail -F /home/hadoop/python/logs/access.log
exec-memory-logger.sources.exec-source.shell = /bin/sh -c

exec-memory-logger.channels.memory-channel.type = memory

exec-memory-logger.sinks.logger-sink.type = logger

exec-memory-logger.sources.exec-source.channels = memory-channel
exec-memory-logger.sinks.logger-sink.channel = memory-channel

```
运行
```shell
flume-ng agent \
--name exec-memory-logger \
--conf $FLUME_HOME/conf \
--conf-file /home/hadoop/project/streaming_project.conf \
-Dflume.root.logger=INFO,console
```

## 将Flume数据转移到Kafka

vim streaming_project2.conf
```conf
exec-memory-kafka.sources = exec-source
exec-memory-kafka.sinks = kafka-sink
exec-memory-kafka.channels = memory-channel

exec-memory-kafka.sources.exec-source.type = exec
exec-memory-kafka.sources.exec-source.command = tail -F /home/hadoop/python/logs/access.log
exec-memory-kafka.sources.exec-source.shell = /bin/sh -c

exec-memory-kafka.channels.memory-channel.type = memory

exec-memory-kafka.sinks.kafka-sink.type = org.apache.flume.sink.kafka.KafkaSink
exec-memory-kafka.sinks.kafka-sink.brokerList = localhost:9092
exec-memory-kafka.sinks.kafka-sink.topic = streaming_topic
exec-memory-kafka.sinks.kafka-sink.batchSize = 5
exec-memory-kafka.sinks.kafka-sink.requiredAcks = 1

exec-memory-kafka.sources.exec-source.channels = memory-channel
exec-memory-kafka.sinks.kafka-sink.channel = memory-channel

```
运行
```shell
flume-ng agent \
--name exec-memory-kafka \
--conf $FLUME_HOME/conf \
--conf-file /home/hadoop/project/streaming_project2.conf \
-Dflume.root.logger=INFO,console
```

创建一个消费者进行测试：`./bin/kafka-console-consumer.sh --zookeeper localhost:2181 --topic streaming_topic`

## Spark Streaming 接收Kafka数据
```java
object ZxlStreamingApp {

  def main(args: Array[String]): Unit = {

    if (args.length != 4) {
      System.err.print("params have mistake.must 4 param")
      System.exit(1)
    }

    val Array(zkQuorum, groupId, topics, numThreads) = args
    val sparkConf = new SparkConf().setAppName("ZxlStreamingApp").setMaster("local[2]")
    val ssc = new StreamingContext(sparkConf, Seconds(6))

    val topicMap = topics.split(",").map((_, numThreads.toInt)).toMap
    val messages = KafkaUtils.createStream(ssc, zkQuorum, groupId, topicMap)

    messages.map(_._2).count().print

    ssc.start()
    ssc.awaitTermination()
  }

}

```

## 统计今天到现在为止的课程访问量

可视化前端数据：yyyyMMdd    courseId    click_count
数据库选择：HBase

### 环境准备
1. 启动hdfs `cd $HADOOP_HOME/sbin && ./start-dfs.sh`
2. 启动hbase `cd $HBASE_HOME/bin && ./start-hbase.sh`,hbase shell `./bin/hbase shell`
3. 创建HBASE表 
    create 'zxl_course_clickcount','info'


### 编写HBASE操作的工具类

hbase-shell 无法使用backspace，需要使用ctrl+backspace
```java
/**
 * HBASE操作工具类
 */
public class HBaseUtils {

    HBaseAdmin admin = null;
    Configuration configuration = null;

    private HBaseUtils() {
        configuration = new Configuration();
        configuration.set("hbase.zookeeper.quorum", "www.chenyuspace.com:2181");
        configuration.set("hbase.rootdir", "hdfs://www.chenyuspace.com:8020/hbase");

        try {
            admin = new HBaseAdmin(configuration);
        } catch (IOException e) {
            e.printStackTrace();
        }
    }

    private static HBaseUtils instance = null;

    public static synchronized HBaseUtils getInstance() {
        if (instance == null) {
            instance = new HBaseUtils();
        }
        return instance;
    }

    /**
     * 根据表名获取到实例
     *
     * @param tableName
     * @return
     */
    public HTable getTable(String tableName) {
        HTable table = null;
        try {
            table = new HTable(configuration, tableName);
        } catch (IOException e) {
            e.printStackTrace();
        }

        return table;
    }

    /**
     * 添加一条记录到hbase
     * @param tableName 表名
     * @param rowKey rowKey行
     * @param cf columnFamily
     * @param column 列
     * @param value 值
     */
    public void put(String tableName,String rowKey,String cf,String column,String value){
        HTable table = getTable(tableName);
        Put put = new Put(Bytes.toBytes(rowKey));
        put.add(Bytes.toBytes(cf),Bytes.toBytes(column),Bytes.toBytes(value));
        try {
            table.put(put);
        } catch (IOException e) {
            e.printStackTrace();
        }
    }

    public static void main(String[] args) {
        //HTable table = getInstance().getTable("zxl_course_clickcount");
        //System.out.println(table.getName().getNameAsString());

        HBaseUtils.getInstance().put("zxl_course_clickcount","20171228_88","info","click_count","2");
    }

}

```

4. 注意事项
配置`hbase-site.xml`时候，如果调用程序使用域名或者IP调用的
```xml
<property>
    <name>hbase.zookeeper.quorum</name>
    <value>119.23.249.139:2181</value>
</property>
```
zookeeper的IP地址一定要填写实际的IP地址


### 编写SCALA的dao层
Scala可以直接调用Java的代码
```java
object CourseClickCountDao {

  val tableName = "zxl_course_clickcount"
  val cf = "info"
  val qualifer = "click_count"

  /**
    * 保存数据到hbase
    *
    * @param list
    */
  def save(list: ListBuffer[CourseClickCount]): Unit = {
    val table = HBaseUtils.getInstance().getTable(tableName)

    for (ele <- list) {
      table.incrementColumnValue(Bytes.toBytes(ele.day_course),
        Bytes.toBytes(cf),
        Bytes.toBytes(qualifer),
        ele.click_count)
    }
  }

  /**
    * 根据课程号查询
    *
    * @param day_course
    */
  def count(day_course: String): Long = {
    val table = HBaseUtils.getInstance().getTable(tableName)
    val get = new Get(Bytes.toBytes(day_course))
    val value = table.get(get).getValue(cf.getBytes, qualifer.getBytes)

    if (value == null) {
      0L
    } else {
      Bytes.toLong(value)
    }
  }

  def main(args: Array[String]): Unit = {
    var list = new ListBuffer[CourseClickCount]
    list.append(CourseClickCount("20170808_8", 8))
    list.append(CourseClickCount("20170808_9", 9))
    list.append(CourseClickCount("20170808_10", 10))

    save(list)
    print(count("20170808_8") + ":" + count("20170808_9") + ":" + count("20170808_10"))
  }
}

```

### 编写业务计算代码
```java

object ZxlStreamingApp {

  def main(args: Array[String]): Unit = {

    if (args.length != 4) {
      System.err.print("params have mistake.must 4 param")
      System.exit(1)
    }

    val Array(zkQuorum, groupId, topics, numThreads) = args
    val sparkConf = new SparkConf().setAppName("ZxlStreamingApp").setMaster("local[2]")
    val ssc = new StreamingContext(sparkConf, Seconds(6))

    val topicMap = topics.split(",").map((_, numThreads.toInt)).toMap
    val messages = KafkaUtils.createStream(ssc, zkQuorum, groupId, topicMap)

    //1.测试数据接收
    //messages.map(_._2).count().print

    //2.数据清洗
    val logs = messages.map(_._2)
    //logs.print()
    val cleanData = logs.map(line => {
      val infos = line.split("\t")
      val url = infos(2).split(" ")(1)
      var courseId = 0

      //3.拿到课程编号
      if (url.startsWith("/class")) {
        val courseIdHTML = url.split("/")(2)
        courseId = courseIdHTML.substring(0, courseIdHTML.lastIndexOf(".")).toInt
      }
      ClickLog(infos(0), DataUtils.parseToMinute(infos(1)), courseId, infos(4).toInt, infos(3))

    }).filter(clickLog => clickLog.courseId != 0)

    cleanData.print()

    //3.统计今天到现在为止的课程访问量
    cleanData.map(x => {
      //将日期转换成 20171228_88格式
      (x.time.substring(0, 8) + "_" + x.courseId, 1)
    }).reduceByKey(_ + _).foreachRDD(rdd => {
      rdd.foreachPartition(partitionRecords => {
        val list = new ListBuffer[CourseClickCount]
        partitionRecords.foreach(pair => {
          list.append(CourseClickCount(pair._1, pair._2))
        })
        CourseClickCountDao.save(list)
      })
    })

    ssc.start()
    ssc.awaitTermination()
  }

}
```

## 统计从搜索引擎过来的流量

1. HBASE表设计

    create 'zxl_course_search_clickcount','info'
2. rowkey
 
    20171228_www.google.com_100 

### coding
```java
/**
  * 搜索引擎过来的实体
  * @param day_search_course
  * @param click_count
  */
case class CourseSearchClickCount(day_search_course:String,click_count:Long)

```

```java

object CourseSearchClickCountDao {

  val tableName = "zxl_course_search_clickcount"
  val cf = "info"
  val qualifer = "click_count"

  /**
    * 保存数据到hbase
    *
    * @param list
    */
  def save(list: ListBuffer[CourseSearchClickCount]): Unit = {
    val table = HBaseUtils.getInstance().getTable(tableName)

    for (ele <- list) {
      table.incrementColumnValue(Bytes.toBytes(ele.day_search_course),
        Bytes.toBytes(cf),
        Bytes.toBytes(qualifer),
        ele.click_count)
    }
  }

  /**
    * 根据课程号查询
    *
    * @param day_search_course
    */
  def count(day_search_course: String): Long = {
    val table = HBaseUtils.getInstance().getTable(tableName)
    val get = new Get(Bytes.toBytes(day_search_course))
    val value = table.get(get).getValue(cf.getBytes, qualifer.getBytes)

    if (value == null) {
      0L
    } else {
      Bytes.toLong(value)
    }
  }

  def main(args: Array[String]): Unit = {
    var list = new ListBuffer[CourseSearchClickCount]
    list.append(CourseSearchClickCount("20171228_www.google.com_100", 100))
    list.append(CourseSearchClickCount("20171228_www.baidu.com_20", 20))
    list.append(CourseSearchClickCount("20171228_www.yahoo.com_50", 50))

    save(list)
    print(count("20171228_www.google.com_100") + ":" + count("20171228_www.baidu.com_20") + ":" + count("20171228_www.yahoo.com_50"))
  }
}
```

```java
/**
  * Created by zxlvoid on 2017/12/28 0028.
  * Spark Streaming 处理Kafka的数据
  */
object ZxlStreamingApp {

  def main(args: Array[String]): Unit = {

    if (args.length != 4) {
      System.err.print("params have mistake.must 4 param")
      System.exit(1)
    }

    val Array(zkQuorum, groupId, topics, numThreads) = args
    //线上环境需要去掉local[2]
    val sparkConf = new SparkConf().setAppName("ZxlStreamingApp")//.setMaster("local[2]")
    val ssc = new StreamingContext(sparkConf, Seconds(6))

    val topicMap = topics.split(",").map((_, numThreads.toInt)).toMap
    val messages = KafkaUtils.createStream(ssc, zkQuorum, groupId, topicMap)

    //1.测试数据接收
    //messages.map(_._2).count().print

    //2.数据清洗
    val logs = messages.map(_._2)
    //logs.print()
    val cleanData = logs.map(line => {
      val infos = line.split("\t")
      val url = infos(2).split(" ")(1)
      var courseId = 0

      //3.拿到课程编号
      if (url.startsWith("/class")) {
        val courseIdHTML = url.split("/")(2)
        courseId = courseIdHTML.substring(0, courseIdHTML.lastIndexOf(".")).toInt
      }
      ClickLog(infos(0), DataUtils.parseToMinute(infos(1)), courseId, infos(4).toInt, infos(3))

    }).filter(clickLog => clickLog.courseId != 0)

    cleanData.print()

    //3.统计今天到现在为止的课程访问量
    cleanData.map(x => {
      //将日期转换成 20171228_88格式
      (x.time.substring(0, 8) + "_" + x.courseId, 1)
    }).reduceByKey(_ + _).foreachRDD(rdd => {
      rdd.foreachPartition(partitionRecords => {
        val list = new ListBuffer[CourseClickCount]
        partitionRecords.foreach(pair => {
          list.append(CourseClickCount(pair._1, pair._2))
        })
        CourseClickCountDao.save(list)
      })
    })

    //4.将搜索引擎过来的访问量进行统计
    cleanData.map(x => {
      /**
        * https://www.google.com/search?q={query}
        * ==>
        * https:/www.google.com/search?q={query}
        **/
      val referer = x.referer.replaceAll("//", "/")
      val splits = referer.split("/")
      var host = ""
      if (splits.length > 2) {
        host = splits(1)
      }

      (host, x.courseId, x.time)
    }).filter(_._1 != "").map(x => {
      (x._3.substring(0, 8) + "_" + x._1 + "_" + x._2, 1)
    }).reduceByKey(_ + _).foreachRDD(rdd => {
      rdd.foreachPartition(partitionRecords => {
        val list = new ListBuffer[CourseSearchClickCount]
        partitionRecords.foreach(pair => {
          list.append(CourseSearchClickCount(pair._1, pair._2))
        })
        CourseSearchClickCountDao.save(list)
      })
    })

    ssc.start()
    ssc.awaitTermination()
  }

}

```

## 线上环境运行spark这个任务